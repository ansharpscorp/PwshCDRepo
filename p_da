def process_day(date_str, cfg):
    """Per-day worker ‚Äî reconnects to DB in each process."""
    from processor import enrich_call_data
    from sqlalchemy import create_engine
    import pandas as pd

    logger = get_logger()
    print(f"üìÑ [{date_str}] Starting day process...")

    try:
        # New engine inside each subprocess (avoids pickle)
        engine = get_sql_engine(cfg)
        print(f"üîó [{date_str}] Connected to SQL inside subprocess.")

        # Read per-day call data only
        query = f"""
        SELECT * FROM {cfg['tables']['call_data']}
        WHERE CONVERT(date, [{cfg['date_column']}]) = '{pd.to_datetime(date_str).strftime('%Y-%m-%d')}'
        """
        call_df = pd.read_sql(query, engine)
        if call_df.empty:
            print(f"‚ö†Ô∏è [{date_str}] No data found ‚Äî skipping.")
            return None, date_str, "No data"

        # Re-read lookup tables inside worker
        user_df = pd.read_sql(f"SELECT * FROM {cfg['tables']['user_data']}", engine)
        network_df = pd.read_sql(f"SELECT * FROM {cfg['tables']['network_data']}", engine)
        print(f"üì• [{date_str}] Lookup tables loaded ({len(user_df)} users, {len(network_df)} subnets).")

        enriched = enrich_call_data(call_df, user_df, network_df)
        print(f"‚úÖ [{date_str}] Processed successfully ({len(enriched)} rows).")
        return enriched, date_str, "Success"

    except Exception as e:
        print(f"‚ùå [{date_str}] Error: {e}")
        logger.error(f"‚ùå [{date_str}] {e}")
        return None, date_str, f"Failed: {e}"


-----------------------------------------------

def process_day(date_str, cfg):
    from processor import enrich_call_data
    from sqlalchemy import create_engine
    import pandas as pd

    logger = get_logger()
    print(f"üìÑ [{date_str}] Starting day process...")

    try:
        engine = get_sql_engine(cfg)
        columns = ", ".join(cfg.get("columns", ["*"]))
        query = f"""
        SELECT {columns}
        FROM {cfg['tables']['call_data']}
        WHERE CONVERT(date, [{cfg['date_column']}]) = '{pd.to_datetime(date_str).strftime('%Y-%m-%d')}'
        """
        print(f"üì¶ [{date_str}] Loading call data in chunks...")
        call_df = pd.concat(pd.read_sql(query, engine, chunksize=100000), ignore_index=True)
        print(f"üì¶ [{date_str}] Loaded {len(call_df):,} rows.")

        if call_df.empty:
            return None, date_str, "No data"

        user_df = pd.read_sql(f"SELECT * FROM {cfg['tables']['user_data']}", engine)
        network_df = pd.read_sql(f"SELECT * FROM {cfg['tables']['network_data']}", engine)

        enriched_parts = []
        for chunk in np.array_split(call_df, 10):
            enriched_parts.append(enrich_call_data(chunk, user_df, network_df))
        enriched = pd.concat(enriched_parts, ignore_index=True)
        del call_df, user_df, network_df

        print(f"‚úÖ [{date_str}] Processed successfully ({len(enriched)} rows).")
        return enriched, date_str, "Success"

    except Exception as e:
        print(f"‚ùå [{date_str}] Error: {e}")
        logger.error(f"‚ùå [{date_str}] {e}")
        return None, date_str, f"Failed: {e}"

